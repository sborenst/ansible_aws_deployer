

- name: Starting environment deployment
  hosts: localhost
  connection: local
  gather_facts: False
  vars_files:
  - "../Environments/{{ config }}_vars.yml"
  - "../Environments/{{ config }}_secret_vars.yml"

  tasks:

######################### Generate CF Template
######################### Launch CF Template

  - name: Provisioning Environment
    gather_facts: False
    vars_files:
    - "../Environments/{{ config }}_vars.yml"
    - "../Environments/{{ config }}_secret_vars.yml"
    include: ./provision_cf.yml
    tags:
      - provisioning
      - gen_cf_templatem
      - provisiong_cf_template



######################### Populate Dynamic Inventory

  - name: Creating Dynamic Inventory
    gather_facts: False
    vars_files:
    - "../Environments/{{ config }}_vars.yml"
    - "../Environments/{{ config }}_secret_vars.yml"
    include: ./dynamic_inventory.yml
    tags:
      - dynamic_inventory
      - get_hosts
      - populate_hosts
      - generate_ansible_host_file


######################### LOCAL Bastion Proxy Config

  - name: Generate ssh.cfg Template
    template:
      src: files/ssh.cfg.j2
      dest: /tmp/ssh-{{ config }}-{{ guid }}.cfg
    tags:
      - bastion_proxy_config
      - create_ssh_cfg_template
## Need to add a check here to see if this was already done
  - name: copy ssh.cfg into local ~.ssh/Config
    shell: "cat /tmp/ssh-{{ config }}-{{ guid }}.cfg >> ~/.ssh/config"
    tags:
      - bastion_proxy_config
      - workaround
      - config_ssh_config_file

######################### Run COMMON Tasks on all hosts

- name: Configuring common Hosts
  hosts: bastions,nfs,masters,infranodes,nodes
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
    - "../Environments/{{ config }}_secret_vars.yml"
  roles:
    - common
  tags:
    - common_tasks


######################### Run bastion Tasks



- name: Configuring Bastion Hosts
  hosts: bastions
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  roles:
    - bastion
  tags:
    - bastion_tasks


#########################Configuring nfs Host

- name: Configuring nfs Host
  hosts: nfs
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  roles:
    - nfs
  tags:
    - nfs_tasks

- name: Configuring nfs Host
  hosts: nfs
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  tags:
    - nfs_tasks
  tasks:
  ## TODO: Use the file module for idempotency
    - shell: 'mkdir -p {{ nfs_export_path }}/user-vols/vol{1..{{ user_vols }}}'
#    - shell: 'chown -R {{ nfs_export_path }}/user-vols/vol{1..{{ user_vols }}}'
    - shell: 'chmod -R 0777 {{ nfs_export_path }}/user-vols/vol{1..{{ user_vols }}}'

####################### Startring OpenShift Specific Deployment

#########################Configuring openshfit-provisioner

- name: Configuring openshfit-provisioner
  hosts: bastions
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  roles:
    - openshift-provisioner
  tags:
    - openshift_provisioner_tasks


#########################Configuring openshfit-nodes

- name: Configuring openshfit-nodes
  hosts: infranodes,nodes
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  roles:
    - openshift-node
  tags:
    - openshift_node_tasks


######################### Run OpenShift Installer


- name: Run OpenShift Installer
  hosts: bastions
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  tags:
    - installing_openshift
  tasks:

    - debug: msg="{{ item }}"
      with_items: name_servers.stdout_lines

#    - name: Was the DNS Domain set for the route53 used
#      pause: prompt="Was the DNS Domain set for the route53 used"

    - name: Add log path to Ansible configuration
      lineinfile:
        regexp: "^#log_path"
        dest: "/etc/ansible/ansible.cfg"
        line: "log_path = /root/ansible.log"
        state: present
## TODO: Use include instead of executing ansible with shell
    - name: run ansible-playbook -i /etc/ansible/hosts /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
      shell: "ansible-playbook -i /etc/ansible/hosts /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml"
      register: openshift_install_log
      tags:
        - openshfit_installer

## TODO: Use include instead of executing ansible with shell
    - name: get openshfit credentials file
      shell: ansible masters[0] -b -m fetch -a "src=/root/.kube/config dest=/root/.kube/config flat=yes"
      tags:
        - get_openshift_credentials

######################### Create user-vols PVs
- name: Create user-vols PVs
  hosts: bastions
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  tags:
    - create_user_vol_pvs
  tasks:
    - set_fact:
        pv_size: '10Gi'
        pv_list: "{{ nfs_shares }}"
        persistentVolumeReclaimPolicy: Retain

    - name: Generate PV file
      template:
        src: files/ocp-ml_pvs.j2
        dest: /root/pvs-{{ config }}-{{ guid }}.yml
      tags:
        - testing

    - set_fact:
        pv_size: "{{ user_vols_size }}"
        persistentVolumeReclaimPolicy: Recycle
    - name: Generate user vol PV file
      template:
        src: files/ocp-ml_userpvs.j2
        dest: /root/userpvs-{{ config }}-{{ guid }}.yml
      tags:
        - testing
## TODO: Remove the become statements. This is already covered
## at the main level play.
    - shell: 'oc create -f /root/pvs-{{ config }}-{{ guid }}.yml || oc update -f /root/pvs-{{ config }}-{{ guid }}.yml'
      become: true

    - shell: 'oc create -f /root/userpvs-{{ config }}-{{ guid }}.yml || oc update -f /root/userpvs-{{ config }}-{{ guid }}.yml'
      become: true


######################### Configure Logging
- name: Configure Logging
  hosts: bastions
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  tags:
    - configure_logging
  tasks:
    - shell: 'oadm new-project logging --node-selector=""'
      ignore_errors: yes

    - shell: ssh {{ hostvars[groups['masters'][0]]['private_hostname'] }} "oc apply -n openshift -f /usr/share/openshift/examples/infrastructure-templates/enterprise/logging-deployer.yaml"

    - shell: 'oc project logging'

    - shell: 'oc new-app logging-deployer-account-template -n logging'
      ignore_errors: yes

    - shell: 'oadm policy add-cluster-role-to-user oauth-editor system:serviceaccount:logging:logging-deployer'

    - shell: 'oadm policy add-scc-to-user privileged system:serviceaccount:logging:aggregated-logging-fluentd'

    - shell: 'oadm policy add-cluster-role-to-user cluster-reader system:serviceaccount:logging:aggregated-logging-fluentd'
  ## TODO: Instead of sleeping for 30, is there
  ## some service, port, etc we can hit to wait for?
  ## Generic "30" could potentially be busted under
  ## heavy load.
    - shell: 'oc new-app logging-deployer-template --param ES_PVC_SIZE=9Gi --param PUBLIC_MASTER_URL=https://{{ master_lb_dns }}:8443 --param KIBANA_HOSTNAME=kibana.{{ cloudapps_suffix }} --param IMAGE_VERSION=3.3.0 --param IMAGE_PREFIX=registry.access.redhat.com/openshift3/        --param KIBANA_NODESELECTOR=region=infra --param ES_NODESELECTOR=region=infra --param MODE=uninstall -n logging'
      ignore_errors: yes

    - shell: "sleep 30"

    - shell: 'oc new-app logging-deployer-template --param ES_PVC_SIZE=9Gi --param PUBLIC_MASTER_URL=https://{{ master_lb_dns }}:8443 --param KIBANA_HOSTNAME=kibana.{{ cloudapps_suffix }} --param IMAGE_VERSION=3.3.0 --param IMAGE_PREFIX=registry.access.redhat.com/openshift3/        --param KIBANA_NODESELECTOR=region=infra --param ES_NODESELECTOR=region=infra --param MODE=install -n logging'

    - shell: 'oc label nodes --all logging-infra-fluentd=true --overwrite=true'

    - shell: 'oc label node -l openshift_schedulable=False --overwrite logging-infra-fluentd=false  --overwrite=true'

    - shell: 'oc project default'

- name: restart master api
  hosts: masters
  become: yes
  vars_files:
    - "../Environments/{{ config }}_vars.yml"
  tags:
    - restart_master_api
  tasks:
    - service:
        name: atomic-openshift-master-api
        state: restarted
